{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fPxfuSuIlBwN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial import Voronoi\n",
    "import kneed\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import distance\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull, Voronoi, distance\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install kneed"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21N6fxYgC9Or",
    "outputId": "7c5b8889-b47b-4429-8f23-a9b90c892ead"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting kneed\n",
      "  Downloading kneed-0.8.3-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.10.1)\n",
      "Installing collected packages: kneed\n",
      "Successfully installed kneed-0.8.3\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2)**2))\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, centroid):\n",
    "        self.centroid = centroid\n",
    "        self.points = []\n",
    "        self.radius = 0.0\n",
    "    \n",
    "    def update_centroid(self):\n",
    "        self.centroid = np.mean(self.points, axis=0)\n",
    "    \n",
    "    def add_point(self, point):\n",
    "        self.points.append(point)\n",
    "    \n",
    "    def update_radius(self, distance):\n",
    "        if distance > self.radius:\n",
    "            self.radius = distance\n",
    "\n",
    "\n",
    "def k_star_algorithm(K_star, K, dataset, N):\n",
    "    # Step 1: Arbitrary choose K_star initial centers\n",
    "    centers = dataset[np.random.choice(len(dataset), K_star, replace=False)]\n",
    "\n",
    "    clusters = [Cluster(center) for center in centers]\n",
    "\n",
    "    for point in dataset:\n",
    "        distances = [euclidean_distance(point, cluster.centroid) for cluster in clusters]\n",
    "        closest_cluster_index = np.argmin(distances)\n",
    "        closest_cluster = clusters[closest_cluster_index]\n",
    "        closest_cluster.add_point(point)\n",
    "\n",
    "    while True:\n",
    "        converged = True\n",
    "\n",
    "        for idx, cluster in enumerate(clusters):\n",
    "            cluster_set = [c for c in clusters if c != cluster]\n",
    "            updated_points = []\n",
    "\n",
    "            for point in cluster.points:\n",
    "                distances = [euclidean_distance(point, c.centroid) for c in cluster_set]\n",
    "                closest_cluster_index = np.argmin(distances)\n",
    "                closest_cluster = cluster_set[closest_cluster_index]\n",
    "                distance_to_closest_cluster = distances[closest_cluster_index]\n",
    "                if distance_to_closest_cluster < euclidean_distance(point, cluster.centroid):\n",
    "\n",
    "                    if distance_to_closest_cluster > cluster.radius + closest_cluster.radius + abs(cluster.radius * closest_cluster.radius):\n",
    "                        continue  # Skip moving point if Lemma 4 condition is not satisfied\n",
    "\n",
    "                    closest_cluster.add_point(point)\n",
    "\n",
    "                    if distance_to_closest_cluster > closest_cluster.radius:\n",
    "                      closest_cluster.radius = distance_to_closest_cluster\n",
    "                    \n",
    "                    # update m'\n",
    "                    closest_cluster.centroid = closest_cluster.centroid + (euclidean_distance(point, closest_cluster.centroid)/(len(closest_cluster.points)+1))\n",
    "                    cluster.centroid = cluster.centroid + (euclidean_distance(point, cluster.centroid)/(len(cluster.points)-1))\n",
    "\n",
    "                elif euclidean_distance(point, cluster.centroid) > cluster.radius:\n",
    "                    cluster.radius = euclidean_distance(point, cluster.centroid)\n",
    "                \n",
    "\n",
    "        if converged:\n",
    "          if len(clusters) > K:\n",
    "            clusters = top_n(clusters, N)\n",
    "          else:\n",
    "            break\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "\n",
    "def top_n(clusters, N):\n",
    "    distances = []\n",
    "\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(i+1, len(clusters)):\n",
    "            distance = euclidean_distance(clusters[i].centroid, clusters[j].centroid)\n",
    "            distances.append((distance, i, j))\n",
    "\n",
    "    distances.sort()\n",
    "    selected_clusters = set()\n",
    "    merged_clusters = []\n",
    "    for distance, i, j in distances[:N]:\n",
    "        if i not in selected_clusters and j not in selected_clusters:\n",
    "\n",
    "            merged_clusters.append(clusters[i].points + clusters[j].points)\n",
    "            selected_clusters.add(i)\n",
    "            selected_clusters.add(j)\n",
    "    \n",
    "    for i in range(len(clusters)):\n",
    "        if i not in selected_clusters:\n",
    "            merged_clusters.append(clusters[i].points)\n",
    "\n",
    "\n",
    "    # form new clusters\n",
    "    new_clusters = []\n",
    "    for new_cluster_points in merged_clusters:\n",
    "        new_cluster = Cluster(np.mean(new_cluster_points, axis=0))\n",
    "        new_cluster.points = new_cluster_points\n",
    "        new_clusters.append(new_cluster)\n",
    "\n",
    "    return new_clusters"
   ],
   "metadata": {
    "id": "OLannFp8OlfF"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MyDBSCAN:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def calculate_hull(self):\n",
    "        hull = ConvexHull(self.data)\n",
    "        return hull\n",
    "\n",
    "    def calculate_voronoi(self):\n",
    "        voronoi = Voronoi(self.data)\n",
    "        return voronoi\n",
    "\n",
    "    def calculate_interiors(self, hull, voronoi):\n",
    "        v_vertices = np.array(voronoi.vertices)\n",
    "        hull_points = np.array(self.data[hull.vertices])\n",
    "        interiors = []\n",
    "        for i in range(len(v_vertices)):\n",
    "            new_array = np.append(hull_points, [v_vertices[i]], axis=0)\n",
    "            convex_hull_to_test = ConvexHull(new_array)\n",
    "\n",
    "            # check if the array is unchanged\n",
    "            new_hull_points = np.array(new_array[convex_hull_to_test.vertices])\n",
    "            if np.array_equal(new_hull_points, hull_points):\n",
    "                interiors.append(v_vertices[i])\n",
    "        return interiors\n",
    "\n",
    "    def calculate_eps_ec(self, interiors):\n",
    "        possible_centers = np.array(interiors)\n",
    "        distances = distance.cdist(possible_centers, self.data, metric='euclidean')\n",
    "        min_distances = [min(distances[i]) for i in range(len(distances))]\n",
    "        radius = np.sort(min_distances)\n",
    "        elbow = KneeLocator(range(len(radius)), radius, curve='convex', direction='increasing')\n",
    "        eps_ec = radius[elbow.knee]\n",
    "        return eps_ec\n",
    "\n",
    "    def find_best_min_points(self, eps_ec):\n",
    "        best_scores = {}\n",
    "        for i in range(2, 100):\n",
    "            dbscan = DBSCAN(eps=eps_ec, min_samples=i)\n",
    "            labels = dbscan.fit_predict(self.data)\n",
    "            if len(set(labels)) > 1:\n",
    "                score = silhouette_score(self.data, labels)\n",
    "                best_scores[score] = [i, eps_ec]\n",
    "            else:\n",
    "                continue\n",
    "        best_min_points = max(best_scores.keys())\n",
    "        return best_scores[best_min_points]\n",
    "\n",
    "    def assign_labels(self, best_min_points):\n",
    "        dbscan = DBSCAN(eps=best_min_points[1], min_samples=best_min_points[0])\n",
    "        labels = dbscan.fit_predict(self.data)\n",
    "        noise = []\n",
    "        unique_labels = np.unique(labels)\n",
    "        centroids = []\n",
    "        for label in unique_labels:\n",
    "            if label != -1:\n",
    "                centroid = np.mean(self.data[labels == label], axis=0)\n",
    "                centroids.append((label, centroid))\n",
    "        for i in range(len(labels)):\n",
    "            distance_centr = []\n",
    "            if labels[i] == -1:\n",
    "                temp = self.data[i]\n",
    "                for j in range(len(centroids)):\n",
    "                    d = distance.euclidean(temp, centroids[j][1])\n",
    "                    distance_centr.append((d, centroids[j][0]))\n",
    "                sorted_distance = sorted(distance_centr, key=lambda x: x[0])\n",
    "                labels[i] = sorted_distance[0][1]\n",
    "        return labels\n",
    "\n",
    "    def run(self):\n",
    "        hull = self.calculate_hull()\n",
    "        voronoi = self.calculate_voronoi()\n",
    "        interiors = self.calculate_interiors(hull, voronoi)\n",
    "        eps_ec = self.calculate_eps_ec(interiors)\n",
    "        best_min_points = self.find_best_min_points(eps_ec)\n",
    "        labels = self.assign_labels(best_min_points)\n",
    "        return labels\n",
    "\n",
    "\n",
    "# Usage\n",
    "data = X  # Assuming X is defined\n",
    "dbscan = MyDBSCAN(data)\n",
    "labels = dbscan.run()"
   ],
   "metadata": {
    "id": "Ga6ICnOinsWp"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class GMM:\n",
    "    def __init__(self, n_components, max_iter=1000):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.comp_names = [index for index in range(self.n_components)]\n",
    "        self.pi = [1/self.n_components for comp in range(self.n_components)]\n",
    "\n",
    "    def multivariate_normal(self, X, mean_vector, covariance_matrix):\n",
    "        return multivariate_normal(mean_vector, covariance_matrix).pdf(X)\n",
    "\n",
    "    def e_step(self, X, means, covariances, phis):\n",
    "        num_gaussians = len(means)\n",
    "        W = np.zeros((num_gaussians, len(X)))\n",
    "        for i in range(num_gaussians):\n",
    "            W[i] = self.multivariate_normal(X, means[i], covariances[i])\n",
    "        W = W / W.sum(axis=0)\n",
    "        return W\n",
    "\n",
    "    def m_step(self, X, W):\n",
    "        num_gaussians = len(W)\n",
    "        m, n = X.shape\n",
    "      \n",
    "        phis = np.zeros(num_gaussians)\n",
    "        means = np.zeros((num_gaussians, n))\n",
    "        covariances = np.zeros((num_gaussians, n, n))\n",
    "      \n",
    "        for j in range(num_gaussians):\n",
    "            phis[j] = np.sum(W[j]) / m\n",
    "            means[j] = np.sum(W[j][:, None] * X, axis=0) / np.sum(W[j])\n",
    "            covariances[j] = np.dot((W[j][:, None] * (X - means[j])).T, (X - means[j])) / np.sum(W[j])\n",
    "        return phis, means, covariances\n",
    "\n",
    "    def fit(self, X):\n",
    "        new_X = np.array_split(X, self.n_components)\n",
    "        self.mean_vector = X[np.random.choice(X.shape[0], size=self.n_components, replace=False)]\n",
    "        self.covariance_matrices = np.ones((self.n_components, X.shape[1], X.shape[1])) * np.identity(X.shape[1])\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            W = self.e_step(X, self.mean_vector, self.covariance_matrices, self.pi)\n",
    "            self.pi, self.mean_vector, self.covariance_matrices = self.m_step(X, W)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        The predicting function\n",
    "        :param X: 2-d array numpy array\n",
    "        The data on which we must predict the clusters\n",
    "        '''\n",
    "        probas = []\n",
    "        for n in range(len(X)):\n",
    "            probas.append([self.multivariate_normal(X[n], self.mean_vector[k], self.covariance_matrices[k])\n",
    "                           for k in range(self.n_components)])\n",
    "        cluster = []\n",
    "        for proba in probas:\n",
    "            cluster.append(self.comp_names[proba.index(max(proba))])\n",
    "        return cluster\n"
   ],
   "metadata": {
    "id": "jmH16AxMJ_TV"
   },
   "execution_count": 18,
   "outputs": []
  }
 ]
}
